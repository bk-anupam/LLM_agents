{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804cea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict, Optional, Annotated\n",
    "from operator import itemgetter\n",
    "\n",
    "# Define the state structure for the graph\n",
    "class AgentState(TypedDict):\n",
    "    # messages is the primary list for interaction history and tool calls/results    \n",
    "    messages: Annotated[List[BaseMessage], itemgetter(\"messages\")]\n",
    "    # Optional: Keep query for logging or specific needs, but flow relies on messages\n",
    "    query: Optional[str]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d3cfa",
   "metadata": {},
   "source": [
    "Okay, let's break down that `AgentState` definition and the roles of `Annotated` and `itemgetter` within the context of LangGraph.\n",
    "\n",
    "**Context: Agent State in LangGraph**\n",
    "\n",
    "* **`AgentState(TypedDict)`**: In LangGraph, the `AgentState` is a dictionary (specifically a `TypedDict` for better type hinting) that holds all the information passed between the different steps (nodes) of your agent graph. As the agent processes information, makes decisions, or calls tools, the `AgentState` is updated.\n",
    "* **State Management**: LangGraph needs to know how to update this state dictionary when a node finishes its execution and returns some data. Should it overwrite existing values? Should it append to lists? Should it merge dictionaries? This is where annotations come in.\n",
    "\n",
    "**Explanation of the Code Line:**\n",
    "\n",
    "```python\n",
    "messages: Annotated[List[BaseMessage], itemgetter(\"messages\")]\n",
    "```\n",
    "\n",
    "1.  **`messages: List[BaseMessage]`**:\n",
    "    * This is standard Python type hinting. It declares that the `AgentState` dictionary has a key named `messages`.\n",
    "    * The value associated with this `messages` key is expected to be a list (`List`) where each element is an instance of `BaseMessage` (or one of its subclasses like `HumanMessage`, `AIMessage`, `ToolMessage` from LangChain). This list typically holds the conversation history.\n",
    "\n",
    "2.  **`Annotated[...]`**:\n",
    "    * Imported from Python's `typing` module (`from typing import Annotated`).\n",
    "    * `Annotated` allows you to add **metadata** to a type hint. It doesn't change the fundamental type (`List[BaseMessage]` here), but it attaches extra information that can be used by libraries or tools â€“ in this case, LangGraph.\n",
    "    * The first argument to `Annotated` is the original type hint (`List[BaseMessage]`).\n",
    "    * Subsequent arguments are the metadata annotations.\n",
    "\n",
    "3.  **`itemgetter(\"messages\")`**:\n",
    "    * Imported from Python's built-in `operator` module (`from operator import itemgetter`).\n",
    "    * `itemgetter(\"messages\")` creates a callable object (like a function). When this callable is given a dictionary (or any object that supports item access), it retrieves the item associated with the key `\"messages\"`. For example, if `d = {\"messages\": [1, 2], \"other_key\": 3}`, then `itemgetter(\"messages\")(d)` would return `[1, 2]`.\n",
    "\n",
    "**Purpose in LangGraph State Management:**\n",
    "\n",
    "When you use `Annotated[SomeType, itemgetter(\"some_key\")]` in your `AgentState` definition, you are providing instructions to LangGraph's state management system:\n",
    "\n",
    "* **Selecting Update Data:** It tells LangGraph: \"When a node in the graph finishes and returns a dictionary (representing a partial state update), look for the key named `\"messages\"` within that returned dictionary. The value associated with *that* key is what should be used to update the `messages` field in the main `AgentState`.\"\n",
    "* **Implicit Accumulation (Common Pattern):** While `itemgetter` itself only *selects* the data, LangGraph's `StateGraph` often has default behaviors for merging state. When it sees a key named `messages` (a common convention for history) which is a list, and it receives new messages selected via `itemgetter(\"messages\")`, the default mechanism is usually to **append** or **extend** the existing list in the state with the new messages returned by the node. This effectively accumulates the conversation history.\n",
    "\n",
    "**Why not just use `messages: List[BaseMessage]`?**\n",
    "\n",
    "Without the `Annotated` metadata, LangGraph might not know how to merge partial updates intelligently. It might default to simply overwriting the entire `messages` list with whatever a node returns for that key, which would lose the conversation history. The annotation provides the necessary hint for proper merging/accumulation.\n",
    "\n",
    "**Alternative (More Explicit Accumulation):**\n",
    "\n",
    "You might also see `operator.add` used in annotations, like:\n",
    "\n",
    "```python\n",
    "messages: Annotated[List[BaseMessage], operator.add]\n",
    "```\n",
    "\n",
    "This is a more explicit way to tell LangGraph to use addition (which for lists means concatenation/appending) to merge the state updates for the `messages` field. Both `itemgetter(\"messages\")` and `operator.add` are common ways to achieve the accumulation of message history in LangGraph state, relying on LangGraph interpreting these annotations to manage the state updates correctly.\n",
    "\n",
    "**In summary:** `Annotated` attaches metadata, and `itemgetter(\"messages\")` is that metadata, instructing LangGraph to specifically look for the `\"messages\"` key in a node's output dictionary to find the data needed to update the main `AgentState`'s `messages` list, usually resulting in the accumulation of conversation messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147b017e",
   "metadata": {},
   "source": [
    "### Execution Flow and State Management (Putting it all together)\n",
    "\n",
    "Let's trace an invocation: app = build_agent(vectordb_instance) followed by app.invoke({\"messages\": [HumanMessage(content=\"What is effort according to the murlis?\")]})\n",
    "\n",
    "1. Initial State: {\"messages\": [HumanMessage(content=\"What is effort according to the murlis?\")], \"query\": None}\n",
    "2. Entry Point -> \"agent\": The graph starts at the agent node. agent_node_runnable is called with the initial state.\n",
    "3. Inside agent_node (1st time):\n",
    "    - messages_for_llm becomes [SystemMessage(...), HumanMessage(...)].\n",
    "    - llm_with_tools is invoked. Let's assume the LLM decides it needs context.\n",
    "    - response becomes AIMessage(content='', tool_calls=[ToolCall(name='retrieve_murli_context', args={'query': 'effort according to murlis'}, id='call_123')]).\n",
    "    - The node returns {\"messages\": [response]}.\n",
    "4. State Update 1: LangGraph receives the return dictionary. It sees the messages key is annotated with itemgetter. It appends the new AIMessage to the existing list.\n",
    "    - New State: {\"messages\": [HumanMessage(...), AIMessage(tool_calls=[...])], \"query\": \"What is effort according to the murlis?\"} (query was also updated).\n",
    "5. Conditional Edge from \"agent\": tools_condition is called. It looks at the last message added (AIMessage(tool_calls=[...])). It finds tool_calls. It returns \"action\".\n",
    "6. Routing -> \"action\": The graph routes execution to the action node (ToolNode).\n",
    "7. Inside action node (ToolNode):\n",
    "    - It receives the current state.\n",
    "    - It examines the last message (AIMessage(tool_calls=[...])).\n",
    "    - It finds a tool_call matching the name of one of its tools (bound_murli_retriever).\n",
    "    - It extracts the arguments ({'query': 'effort according to murlis'}).\n",
    "    - It calls bound_murli_retriever(query='effort according to murlis'). Remember, vectordb was already bound using partial.\n",
    "    - Let's say the tool returns the context string: \"Murli excerpt about effort...\".\n",
    "    - ToolNode wraps this result in a ToolMessage: ToolMessage(content=\"Murli excerpt about effort...\", tool_call_id='call_123').\n",
    "    - The node returns {\"messages\": [ToolMessage(...)]}.\n",
    "8. State Update 2: LangGraph receives the return dictionary. It appends the new ToolMessage.\n",
    "    - New State: {\"messages\": [HumanMessage(...), AIMessage(tool_calls=[...]), ToolMessage(...)]}.\n",
    "9. Edge from \"action\": The graph follows the unconditional edge action -> agent.\n",
    "10. Routing -> \"agent\": Execution goes back to the agent node. agent_node_runnable is called again, but this time with the updated state.\n",
    "11. Inside agent_node (2nd time):\n",
    "    - messages_for_llm becomes [SystemMessage(...), HumanMessage(...), AIMessage(tool_calls=[...]), ToolMessage(...)].\n",
    "    - llm_with_tools is invoked with this richer history, including the tool result.\n",
    "    - The LLM now sees the original question and the retrieved context. It generates a final answer.\n",
    "    - response becomes AIMessage(content=\"Based on the murli excerpt, effort means...\"). It has no tool_calls this time.\n",
    "    - The node returns {\"messages\": [response]}.\n",
    "12. State Update 3: LangGraph appends the new final AIMessage.\n",
    "    - New State: {\"messages\": [HumanMessage(...), AIMessage(tool_calls=[...]), ToolMessage(...), AIMessage(content=\"...\")]}.\n",
    "13. Conditional Edge from \"agent\": tools_condition is called again. It looks at the last message (AIMessage(content=\"...\")). It finds no tool_calls. It returns END.\n",
    "14. Routing -> END: The graph execution terminates.\n",
    "15. Final Result: The invoke call returns the final state dictionary. The answer is typically extracted from the content of the last AIMessage in the messages list.\n",
    "\n",
    "Summary of LangGraph Mechanics:\n",
    "\n",
    "- State: A central dictionary (AgentState) holds all information.\n",
    "- Nodes: Functions (agent_node) or pre-built objects (ToolNode) that receive the current state and return a dictionary representing updates to the state.\n",
    "- State Updates: LangGraph merges the dictionary returned by a node into the main state. Special annotations (Annotated[..., itemgetter(...)]) control how updates happen (e.g., append vs. overwrite).\n",
    "- Edges: Define the possible paths between nodes.\n",
    "- Conditional Edges: Use functions (like tools_condition) that inspect the current state (usually the most recent update) to decide which path to take next.\n",
    "- Messages: The primary way information (user input, LLM thoughts/calls, tool results, final answers) flows through the system, stored sequentially in the messages list within the state.\n",
    "- Compilation: Turns the graph definition into an optimized, runnable object."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
